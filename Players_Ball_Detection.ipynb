{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rich==13.0.1\n",
    "%pip install -q gdown inference-gpu --upgrade\n",
    "%pip uninstall -y pymc\n",
    "%pip install pymc==5.12.0\n",
    "%pip install -q onnxruntime-gpu==1.18.0 --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-cache-dir package_name\n",
    "%pip install -q git+https://github.com/roboflow/sports.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q supervision>=0.23.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy==1.26.0\n",
    "%pip install inference-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"C:/Users/user/Downloads/inference-0.43.0-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import get_model\n",
    "#from google.colab import userdata\n",
    "\n",
    "ROBOFLOW_API_KEY = \"ITfUpuY5QO9WTBpcEXTh\"  # Replace with your actual Roboflow API key\n",
    "\n",
    "PLAYER_DETECTION_MODEL_ID = \"football-players-detection-3zvbc/12\"\n",
    "PLAYER_DETECTION_MODEL = get_model(model_id=PLAYER_DETECTION_MODEL_ID, api_key=ROBOFLOW_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple annotation with Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_VIDEO_PATH = \"C:/Users/user/Desktop/FootballAI/videos/goalMatch.mp4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
    "    thickness=2\n",
    ")\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
    "    text_color=sv.Color.from_hex('#000000')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frame_number = 0  # Change to the desired frame number\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "\n",
    "for i, frame in enumerate(frame_generator):\n",
    "    if i == frame_number:\n",
    "        # Run inference on the specific frame\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
    "        detections = sv.Detections.from_inference(result)\n",
    "\n",
    "        # Annotate the frame\n",
    "        annotated_frame = frame.copy()\n",
    "        annotated_frame = box_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections\n",
    "        )\n",
    "        \n",
    "        labels = [\n",
    "          f\"{class_name} {confidence:.2f}\"\n",
    "          for class_name, confidence\n",
    "          in zip(detections[\"class_name\"] , detections.confidence)\n",
    "         ]\n",
    "        annotated_frame = label_annotator.annotate(annotated_frame , detections,labels =labels)##annotate the frame with the labels\n",
    "\n",
    "         # Show the annotated frame\n",
    "        sv.plot_image(annotated_frame)\n",
    "        break  # Stop after processing the specified frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATING VIDEO WITH ANNOTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import supervision as sv\n",
    "\n",
    "SOURCE_VIDEO_PATH = \"C:/Users/user/Desktop/FootballAI/121364_0.mp4\"\n",
    "TARGET_VIDEO_PATH = \"C:/Users/user/Desktop/FootballAI/121364_0_result.mp4\"\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(##Box annotator options like coloring and thickness\n",
    "    color = sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
    "    thickness = 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)##I will take the info from the source video such as resolution and frame rate to pass into the result video \n",
    "video_sink = sv.VideoSink(TARGET_VIDEO_PATH, video_info=video_info)##The result video\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "with video_sink:\n",
    "     for frame in tqdm(frame_generator , total = video_info.total_frames):\n",
    "\n",
    "\n",
    "\n",
    "      label_annotator = sv.LabelAnnotator(##labeling the box and adding text\n",
    "          color = sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
    "          text_color = sv.Color.from_hex('#000000')\n",
    "      )\n",
    "\n",
    "      frame = next(frame_generator)\n",
    "\n",
    "      result = PLAYER_DETECTION_MODEL.infer(frame,confidence = 0.3)[0]##This give me a row data of the detection\n",
    "      detections = sv.Detections.from_inference(result)##Converting the row data into standarized format\n",
    "\n",
    "\n",
    "      labels = [\n",
    "          f\"{class_name} {confidence:.2f}\"\n",
    "          for class_name, confidence\n",
    "          in zip(detections[\"class_name\"] , detections.confidence)\n",
    "      ]\n",
    "\n",
    "      annotated_frame = frame.copy()##Copying the frame to anotate it with boxes\n",
    "      annotated_frame = box_annotator.annotate(annotated_frame , detections)##annotate the frame with the detections\n",
    "      annotated_frame = label_annotator.annotate(annotated_frame , detections,labels =labels)##annotate the frame with the labels\n",
    "      video_sink.write_frame(annotated_frame)##writing the annotated frame and looping over all video frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_ball_to_player(players, ball_bbox):\n",
    "    \"\"\"\n",
    "    Assigns the ball to the closest player based on bounding box center distance.\n",
    "    Returns the index of the player controlling the ball, or -1 if no player is close enough.\n",
    "    \"\"\"\n",
    "    ball_center = np.array([(ball_bbox[0] + ball_bbox[2]) / 2, (ball_bbox[1] + ball_bbox[3]) / 2])\n",
    "    min_distance = float('inf')\n",
    "    assigned_player = -1\n",
    "    \n",
    "    for i, player in enumerate(players):\n",
    "        player_bbox = player['bounding_box']\n",
    "        player_center = np.array([(player_bbox[0] + player_bbox[2]) / 2, (player_bbox[1] + player_bbox[3]) / 2])\n",
    "        distance = np.linalg.norm(player_center - ball_center)\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            assigned_player = i\n",
    "    \n",
    "    return assigned_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting crops: 0it [00:06, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m BALL_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m PLAYER_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 9\u001b[0m crops \u001b[38;5;241m=\u001b[39m \u001b[43mextract_crops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSOURCE_VIDEO_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m team_classifier \u001b[38;5;241m=\u001b[39m TeamClassifier(device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[0;32m     12\u001b[0m team_classifier\u001b[38;5;241m.\u001b[39mfit(crops)\n",
      "Cell \u001b[1;32mIn[32], line 12\u001b[0m, in \u001b[0;36mextract_crops\u001b[1;34m(source_video_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m crops \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m tqdm(frame_generator , desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting crops\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 12\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mPLAYER_DETECTION_MODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     13\u001b[0m     detections \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mDetections\u001b[38;5;241m.\u001b[39mfrom_inference(result)\n\u001b[0;32m     14\u001b[0m     detections \u001b[38;5;241m=\u001b[39m detections\u001b[38;5;241m.\u001b[39mwith_nms(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m , class_agnostic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\models\\object_detection_base.py:84\u001b[0m, in \u001b[0;36mObjectDetectionBaseOnnxRoboflowInferenceModel.infer\u001b[1;34m(self, image, class_agnostic_nms, confidence, disable_preproc_auto_orient, disable_preproc_contrast, disable_preproc_grayscale, disable_preproc_static_crop, iou_threshold, fix_batch_size, max_candidates, max_detections, return_image_dims, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minfer\u001b[39m(\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     44\u001b[0m     image: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     57\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    Runs object detection inference on one or multiple images and returns the detections.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m        ValueError: If batching is not enabled for the model and more than one image is passed for processing.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_agnostic_nms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_agnostic_nms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_preproc_auto_orient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_preproc_auto_orient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_preproc_contrast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_preproc_contrast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_preproc_grayscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_preproc_grayscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_preproc_static_crop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_preproc_static_crop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miou_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_detections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_detections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_image_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_image_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\models\\roboflow.py:732\u001b[0m, in \u001b[0;36mOnnxRoboflowInferenceModel.infer\u001b[1;34m(self, image, **kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m max_batch_size \u001b[38;5;241m=\u001b[39m MAX_BATCH_SIZE \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatching_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (input_elements \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (max_batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference will be executed in batches, as there is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_elements\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input elements and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximum batch size for a model is set to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    736\u001b[0m )\n\u001b[0;32m    737\u001b[0m inference_results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\usage_tracking\\collector.py:674\u001b[0m, in \u001b[0;36mUsageCollector.__call__.<locals>.decorator.<locals>.sync_wrapper\u001b[1;34m(usage_fps, usage_api_key, usage_workflow_id, usage_workflow_preview, usage_inference_test_run, usage_billable, *args, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msync_wrapper\u001b[39m(\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    672\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    673\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 674\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m     t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord_usage(\n\u001b[0;32m    677\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_usage_params_from_func_kwargs(\n\u001b[0;32m    678\u001b[0m             usage_fps\u001b[38;5;241m=\u001b[39musage_fps,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    689\u001b[0m         )\n\u001b[0;32m    690\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\models\\base.py:29\u001b[0m, in \u001b[0;36mBaseInference.infer\u001b[1;34m(self, image, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m preproc_image, returned_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(image, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessed input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(preproc_image,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 29\u001b[0m predicted_arrays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreproc_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m postprocessed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(predicted_arrays, returned_metadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m postprocessed\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\models\\yolov8\\yolov8_object_detection.py:44\u001b[0m, in \u001b[0;36mYOLOv8ObjectDetection.predict\u001b[1;34m(self, img_in, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_in: ImageMetaType, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Performs object detection on the given image using the ONNX session.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mrun_session_via_iobinding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_in\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     47\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m predictions[:, :, :\u001b[38;5;241m4\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\utils\\onnx.py:36\u001b[0m, in \u001b[0;36mrun_session_via_iobinding\u001b[1;34m(session, input_name, input_data)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_session_via_iobinding\u001b[39m(\n\u001b[0;32m     31\u001b[0m     session: ort\u001b[38;5;241m.\u001b[39mInferenceSession, input_name: \u001b[38;5;28mstr\u001b[39m, input_data: ImageMetaType\n\u001b[0;32m     32\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_data, (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;66;03m# skip the iobinding and just run the session\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# we likely won't get any gains by pointing to the input data directly\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDAExecutionProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget_providers():\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;66;03m# no point in doing iobinding as the input must live on CPU anyway\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         input_data \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     40\u001b[0m             input_data\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     41\u001b[0m         )  \u001b[38;5;66;03m# since we must be a tensor but ONNX needs a numpy array\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[0;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import supervision as sv\n",
    "from TeamClassifier import TeamClassifier\n",
    "\n",
    "SOURCE_VIDEO_PATH = \"C:/Users/user/Desktop/FootballAI/121364_0.mp4\"\n",
    "\n",
    "BALL_ID = 0\n",
    "PLAYER_ID = 2\n",
    "\n",
    "crops = extract_crops(SOURCE_VIDEO_PATH)\n",
    "\n",
    "team_classifier = TeamClassifier(device=DEVICE)\n",
    "team_classifier.fit(crops)\n",
    "\n",
    "ellipse_annotator = sv.EllipseAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "    thickness=2\n",
    ")\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "    text_color=sv.Color.from_hex('#000000'),\n",
    "    text_position=sv.Position.BOTTOM_CENTER\n",
    ")\n",
    "triangle_annotator = sv.TriangleAnnotator(\n",
    "    color=sv.Color.from_hex('#FFD700'),\n",
    "    base=25,\n",
    "    height=21,\n",
    "    outline_thickness=1\n",
    ")\n",
    "\n",
    "tracker = sv.ByteTrack()\n",
    "tracker.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tracker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m all_detections \u001b[38;5;241m=\u001b[39m detections[detections\u001b[38;5;241m.\u001b[39mclass_id \u001b[38;5;241m!=\u001b[39m BALL_ID]\n\u001b[0;32m     11\u001b[0m all_detections \u001b[38;5;241m=\u001b[39m all_detections\u001b[38;5;241m.\u001b[39mwith_nms(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, class_agnostic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m all_detections \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241m.\u001b[39mupdate_with_detections(detections\u001b[38;5;241m=\u001b[39mall_detections)\n\u001b[0;32m     14\u001b[0m players_detections \u001b[38;5;241m=\u001b[39m all_detections[all_detections\u001b[38;5;241m.\u001b[39mclass_id \u001b[38;5;241m==\u001b[39m PLAYER_ID]\n\u001b[0;32m     15\u001b[0m players_crops \u001b[38;5;241m=\u001b[39m [sv\u001b[38;5;241m.\u001b[39mcrop_image(frame,xyxy) \u001b[38;5;28;01mfor\u001b[39;00m xyxy \u001b[38;5;129;01min\u001b[39;00m players_detections\u001b[38;5;241m.\u001b[39mxyxy]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tracker' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "\n",
    "ball_detections = detections[detections.class_id == BALL_ID]\n",
    "ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
    "\n",
    "all_detections = detections[detections.class_id != BALL_ID]\n",
    "all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
    "all_detections = tracker.update_with_detections(detections=all_detections)\n",
    "\n",
    "players_detections = all_detections[all_detections.class_id == PLAYER_ID]\n",
    "players_crops = [sv.crop_image(frame,xyxy) for xyxy in players_detections.xyxy]\n",
    "players_detections.class_id = team_classifier.predict(players_crops)\n",
    "\n",
    "labels = [\n",
    "    f\"#{tracker_id}\"\n",
    "    for tracker_id\n",
    "    in players_detections.tracker_id\n",
    "]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = ellipse_annotator.annotate( scene=annotated_frame, detections = players_detections)\n",
    "annotated_frame = label_annotator.annotate(scene=annotated_frame,detections = players_detections, labels=labels)\n",
    "annotated_frame = triangle_annotator.annotate( scene=annotated_frame, detections = ball_detections)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFY THE TEAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "SOURCE_VIDEO_PATH = \"C:/Users/user/Desktop/FootballAI/121364_0.mp4\"\n",
    "\n",
    "STRIDE = 30 \n",
    "PLAYER_ID =2\n",
    "def extract_crops(source_video_path : str):\n",
    "        frame_generator = sv.get_video_frames_generator(source_video_path, stride=STRIDE)##Getting the frames with a stride of 30\n",
    "\n",
    "        crops = []\n",
    "\n",
    "        for frame in tqdm(frame_generator , desc=\"Extracting crops\"):\n",
    "            result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.5)[0]\n",
    "            detections = sv.Detections.from_inference(result)\n",
    "            detections = detections.with_nms(threshold=0.5 , class_agnostic=True)\n",
    "            detections = detections[detections.class_id == PLAYER_ID]\n",
    "\n",
    "            crops += [\n",
    "                sv.crop_image(frame,xyxy)\n",
    "                for xyxy\n",
    "                in detections.xyxy\n",
    "            ]\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops = extract_crops(SOURCE_VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "sv.plot_images_grid(crops[:100] , grid_size=(10,10) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch\n",
    "#%pip install transformers\n",
    "%pip install sentencepiece\n",
    "import sentencepiece\n",
    "import torch\n",
    "from transformers import AutoProcessor , SiglipVisionModel\n",
    "\n",
    "SIGLIP_MODEL_PATH = 'google/siglip-base-patch16-224'\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "EMBEDDING_MODEL = SiglipVisionModel.from_pretrained(SIGLIP_MODEL_PATH).to(DEVICE)\n",
    "EMBEDDING_PROCESSOR = AutoProcessor.from_pretrained(SIGLIP_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Number of valid crops: {len(crops)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install more-itertools\n",
    "from more_itertools import chunked\n",
    "import numpy as np\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "crops = [sv.cv2_to_pillow(crop) for crop in crops]\n",
    "batches = chunked(crops , BATCH_SIZE)\n",
    "\n",
    "data = []\n",
    "with torch.no_grad():\n",
    "        for batch in tqdm(batches , desc = 'Extracting embeddings'):\n",
    "            inputs = EMBEDDING_PROCESSOR(images = batch , return_tensors = 'pt').to(DEVICE)\n",
    "            output = EMBEDDING_MODEL(**inputs)\n",
    "\n",
    "            embeddings = torch.mean(output.last_hidden_state , dim = 1).cpu().numpy()\n",
    "            data.append(embeddings)\n",
    "data = np.concatenate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install umap-learn\n",
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "REDUCER = umap.UMAP(n_components=3)\n",
    "CLUSTERING_MODEL = KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections = REDUCER.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = CLUSTERING_MODEL.fit_predict(projections)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
