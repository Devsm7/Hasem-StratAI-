{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abfc1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/09/25 23:24:17] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Your inference package version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.43</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> is out of date! Please upgrade to <a href=\"file://c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\__init__.py#41\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.46</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of inference for the latest features and bug fixes by    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         running `pip install --upgrade inference`.                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/09/25 23:24:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Your inference package version \u001b[1;36m0.43\u001b[0m.\u001b[1;36m0\u001b[0m is out of date! Please upgrade to \u001b]8;id=266414;file://c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=694366;file://c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\__init__.py#41\u001b\\\u001b[2m41\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         version \u001b[1;36m0.46\u001b[0m.\u001b[1;36m3\u001b[0m of inference for the latest features and bug fixes by    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         running `pip install --upgrade inference`.                              \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "ModelDependencyMissing: Your `inference` configuration does not support PaliGemma model. Use pip install 'inference[transformers]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Florence2 model. Use pip install 'inference[transformers]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Qwen2.5-VL model. Use pip install 'inference[transformers]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[clip]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Gaze Detection model. Use pip install 'inference[gaze]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support GroundingDINO model. Use pip install 'inference[grounding-dino]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support YoloWorld model. Use pip install 'inference[yolo-world]' to install missing requirements.\n",
      "UserWarning: Specified provider 'OpenVINOExecutionProvider' is not in available provider names.Available providers: 'TensorrtExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider'\n",
      "UserWarning: Specified provider 'CoreMLExecutionProvider' is not in available provider names.Available providers: 'TensorrtExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider'\n"
     ]
    }
   ],
   "source": [
    "from inference import get_model\n",
    "#from google.colab import userdata\n",
    "\n",
    "ROBOFLOW_API_KEY = \"ITfUpuY5QO9WTBpcEXTh\"  # Replace with your actual Roboflow API key\n",
    "\n",
    "PLAYER_DETECTION_MODEL_ID = \"football-players-detection-3zvbc/12\"\n",
    "PLAYER_DETECTION_MODEL = get_model(model_id=PLAYER_DETECTION_MODEL_ID, api_key=ROBOFLOW_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1034640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_VIDEO_PATH = \"C:/Users/user/Desktop/FootballAI/videos/121364_0.mp4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b75fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from TeamClassifier import TeamClassifier\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b65f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting crops: 134it [18:54,  8.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m BALL_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     13\u001b[0m PLAYER_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 15\u001b[0m crops \u001b[38;5;241m=\u001b[39m \u001b[43mextract_crops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSOURCE_VIDEO_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m team_classifier \u001b[38;5;241m=\u001b[39m TeamClassifier(device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[0;32m     18\u001b[0m team_classifier\u001b[38;5;241m.\u001b[39mfit(crops)\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mextract_crops\u001b[1;34m(source_video_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m crops \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m tqdm(frame_generator , desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting crops\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mPLAYER_DETECTION_MODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m     detections \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mDetections\u001b[38;5;241m.\u001b[39mfrom_inference(result)\n\u001b[0;32m      9\u001b[0m     detections \u001b[38;5;241m=\u001b[39m detections\u001b[38;5;241m.\u001b[39mwith_nms(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m , class_agnostic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\models\\object_detection_base.py:84\u001b[0m, in \u001b[0;36mObjectDetectionBaseOnnxRoboflowInferenceModel.infer\u001b[1;34m(self, image, class_agnostic_nms, confidence, disable_preproc_auto_orient, disable_preproc_contrast, disable_preproc_grayscale, disable_preproc_static_crop, iou_threshold, fix_batch_size, max_candidates, max_detections, return_image_dims, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minfer\u001b[39m(\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     44\u001b[0m     image: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     57\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    Runs object detection inference on one or multiple images and returns the detections.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m        ValueError: If batching is not enabled for the model and more than one image is passed for processing.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_agnostic_nms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_agnostic_nms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_preproc_auto_orient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_preproc_auto_orient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_preproc_contrast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_preproc_contrast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_preproc_grayscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_preproc_grayscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_preproc_static_crop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_preproc_static_crop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miou_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_detections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_detections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_image_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_image_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\models\\roboflow.py:732\u001b[0m, in \u001b[0;36mOnnxRoboflowInferenceModel.infer\u001b[1;34m(self, image, **kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m max_batch_size \u001b[38;5;241m=\u001b[39m MAX_BATCH_SIZE \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatching_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (input_elements \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (max_batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference will be executed in batches, as there is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_elements\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input elements and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximum batch size for a model is set to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    736\u001b[0m )\n\u001b[0;32m    737\u001b[0m inference_results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\usage_tracking\\collector.py:674\u001b[0m, in \u001b[0;36mUsageCollector.__call__.<locals>.decorator.<locals>.sync_wrapper\u001b[1;34m(usage_fps, usage_api_key, usage_workflow_id, usage_workflow_preview, usage_inference_test_run, usage_billable, *args, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msync_wrapper\u001b[39m(\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    672\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    673\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 674\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m     t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord_usage(\n\u001b[0;32m    677\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_usage_params_from_func_kwargs(\n\u001b[0;32m    678\u001b[0m             usage_fps\u001b[38;5;241m=\u001b[39musage_fps,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    689\u001b[0m         )\n\u001b[0;32m    690\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\models\\base.py:29\u001b[0m, in \u001b[0;36mBaseInference.infer\u001b[1;34m(self, image, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m preproc_image, returned_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(image, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessed input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(preproc_image,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 29\u001b[0m predicted_arrays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreproc_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m postprocessed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(predicted_arrays, returned_metadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m postprocessed\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\models\\yolov8\\yolov8_object_detection.py:44\u001b[0m, in \u001b[0;36mYOLOv8ObjectDetection.predict\u001b[1;34m(self, img_in, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_in: ImageMetaType, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Performs object detection on the given image using the ONNX session.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mrun_session_via_iobinding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_in\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     47\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m predictions[:, :, :\u001b[38;5;241m4\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\utils\\onnx.py:36\u001b[0m, in \u001b[0;36mrun_session_via_iobinding\u001b[1;34m(session, input_name, input_data)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_session_via_iobinding\u001b[39m(\n\u001b[0;32m     31\u001b[0m     session: ort\u001b[38;5;241m.\u001b[39mInferenceSession, input_name: \u001b[38;5;28mstr\u001b[39m, input_data: ImageMetaType\n\u001b[0;32m     32\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_data, (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;66;03m# skip the iobinding and just run the session\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# we likely won't get any gains by pointing to the input data directly\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDAExecutionProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget_providers():\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;66;03m# no point in doing iobinding as the input must live on CPU anyway\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         input_data \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     40\u001b[0m             input_data\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     41\u001b[0m         )  \u001b[38;5;66;03m# since we must be a tensor but ONNX needs a numpy array\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[0;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "STRIDE = 30 \n",
    "\n",
    "BALL_ID = 0\n",
    "PLAYER_ID = 2\n",
    "\n",
    "crops = extract_crops(SOURCE_VIDEO_PATH)\n",
    "\n",
    "team_classifier = TeamClassifier(device=DEVICE)\n",
    "team_classifier.fit(crops)\n",
    "\n",
    "ellipse_annotator = sv.EllipseAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "    thickness=2\n",
    ")\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "    text_color=sv.Color.from_hex('#000000'),\n",
    "    text_position=sv.Position.BOTTOM_CENTER\n",
    ")\n",
    "triangle_annotator = sv.TriangleAnnotator(\n",
    "    color=sv.Color.from_hex('#FFD700'),\n",
    "    base=25,\n",
    "    height=21,\n",
    "    outline_thickness=1\n",
    ")\n",
    "\n",
    "tracker = sv.ByteTrack()\n",
    "tracker.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f308f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)\n",
    "\n",
    "result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "\n",
    "ball_detections = detections[detections.class_id == BALL_ID]\n",
    "ball_detections.xyxy = sv.pad_boxes(xyxy=ball_detections.xyxy, px=10)\n",
    "\n",
    "all_detections = detections[detections.class_id != BALL_ID]\n",
    "all_detections = all_detections.with_nms(threshold=0.5, class_agnostic=True)\n",
    "all_detections = tracker.update_with_detections(detections=all_detections)\n",
    "\n",
    "players_detections = all_detections[all_detections.class_id == PLAYER_ID]\n",
    "players_crops = [sv.crop_image(frame,xyxy) for xyxy in players_detections.xyxy]\n",
    "players_detections.class_id = team_classifier.predict(players_crops)\n",
    "\n",
    "labels = [\n",
    "    f\"#{tracker_id}\"\n",
    "    for tracker_id\n",
    "    in players_detections.tracker_id\n",
    "]\n",
    "\n",
    "annotated_frame = frame.copy()\n",
    "annotated_frame = ellipse_annotator.annotate( scene=annotated_frame, detections = players_detections)\n",
    "annotated_frame = label_annotator.annotate(scene=annotated_frame,detections = players_detections, labels=labels)\n",
    "annotated_frame = triangle_annotator.annotate( scene=annotated_frame, detections = ball_detections)\n",
    "\n",
    "sv.plot_image(annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9103da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_crops(source_video_path : str):\n",
    "        frame_generator = sv.get_video_frames_generator(source_video_path, stride=STRIDE)##Getting the frames with a stride of 30\n",
    "\n",
    "        crops = []\n",
    "\n",
    "        for frame in tqdm(frame_generator , desc=\"Extracting crops\"):\n",
    "            result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.5)[0]\n",
    "            detections = sv.Detections.from_inference(result)\n",
    "            detections = detections.with_nms(threshold=0.5 , class_agnostic=True)\n",
    "            detections = detections[detections.class_id == PLAYER_ID]\n",
    "\n",
    "            crops += [\n",
    "                sv.crop_image(frame,xyxy)\n",
    "                for xyxy\n",
    "                in detections.xyxy\n",
    "            ]\n",
    "        return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "787bf8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from TeamClassifier import TeamClassifier\n",
    "import torch\n",
    "\n",
    "def track_ball_possession(source_video_path: str):\n",
    "    frame_generator = sv.get_video_frames_generator(source_video_path, stride=STRIDE)\n",
    "    \n",
    "    possession_tracker = defaultdict(int)  # {team_id: frame_count}\n",
    "    last_team = None  # Keep track of which team had possession in the last frame\n",
    "    consecutive_frames = 0  # Track how long the same team has possession\n",
    "\n",
    "    for i, frame in enumerate(tqdm(frame_generator, desc=\"Tracking ball possession\")):\n",
    "        if i >= 3:  # Process only the first 3 frames\n",
    "            break\n",
    "\n",
    "        result = PLAYER_DETECTION_MODEL.infer(frame, confidence=0.5)[0]\n",
    "        detections = sv.Detections.from_inference(result)\n",
    "        \n",
    "        # Separate ball and players detections\n",
    "        ball_detections = detections[detections.class_id == BALL_ID]\n",
    "        players_detections = detections[detections.class_id == PLAYER_ID]\n",
    "\n",
    "        if len(ball_detections) == 0 or len(players_detections) == 0:\n",
    "            continue  # Skip frame if no ball or players detected\n",
    "\n",
    "        ball_center = get_center(ball_detections.xyxy[0])  # Assuming one ball\n",
    "\n",
    "        crops = extract_crops(SOURCE_VIDEO_PATH)\n",
    "        team_classifier = TeamClassifier(device=DEVICE)\n",
    "        team_classifier.fit(crops)\n",
    "        \n",
    "        # Assign team to players\n",
    "        players_crops = [sv.crop_image(frame, xyxy) for xyxy in players_detections.xyxy]\n",
    "        players_detections.class_id = team_classifier.predict(players_crops)  # Assign team ID\n",
    "\n",
    "        # Find closest player to ball\n",
    "        closest_player, closest_distance = None, float(\"inf\")\n",
    "        for player, xyxy in zip(players_detections.class_id, players_detections.xyxy):\n",
    "            player_center = get_center(xyxy)\n",
    "            distance = get_distance(ball_center, player_center)\n",
    "            if distance < closest_distance:\n",
    "                closest_distance = distance\n",
    "                closest_player = player  # This will be the team ID\n",
    "\n",
    "        if closest_player is not None:\n",
    "            if closest_player == last_team:\n",
    "                consecutive_frames += 1  # Increment possession time\n",
    "            else:\n",
    "                if last_team is not None:\n",
    "                    possession_tracker[last_team] += consecutive_frames\n",
    "                last_team = closest_player\n",
    "                consecutive_frames = 1  # Reset count for new team\n",
    "\n",
    "    # Store the last team's possession count\n",
    "    if last_team is not None:\n",
    "        possession_tracker[last_team] += consecutive_frames\n",
    "\n",
    "    return possession_tracker  # Dictionary of {team_id: possession_frames}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b6988ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_center(xyxy):\n",
    "    \"\"\"Calculate the center (x, y) of a bounding box.\"\"\"\n",
    "    x_min, y_min, x_max, y_max = xyxy\n",
    "    return ((x_min + x_max) / 2, (y_min + y_max) / 2)\n",
    "\n",
    "\n",
    "def get_distance(p1, p2):\n",
    "    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n",
    "    return np.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d9956d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking ball possession: 0it [00:00, ?it/s]\n",
      "Extracting crops: 0it [00:00, ?it/s]\n",
      "Extracting crops: 1it [00:10, 10.62s/it]\n",
      "Extracting crops: 2it [00:23, 11.85s/it]\n",
      "Extracting crops: 3it [00:34, 11.72s/it]\n",
      "Extracting crops: 4it [00:44, 10.99s/it]\n",
      "Extracting crops: 5it [00:53, 10.16s/it]\n",
      "Extracting crops: 6it [01:02,  9.73s/it]\n",
      "Extracting crops: 7it [01:13, 10.21s/it]\n",
      "Extracting crops: 8it [01:25, 10.82s/it]\n",
      "Extracting crops: 9it [01:36, 10.94s/it]\n",
      "Extracting crops: 10it [01:45, 10.31s/it]\n",
      "Extracting crops: 11it [01:54,  9.92s/it]\n",
      "Extracting crops: 12it [02:04,  9.79s/it]\n",
      "Extracting crops: 13it [02:13,  9.64s/it]\n",
      "Extracting crops: 14it [02:22,  9.45s/it]\n",
      "Extracting crops: 15it [02:31,  9.29s/it]\n",
      "Extracting crops: 16it [02:40,  9.31s/it]\n",
      "Extracting crops: 17it [02:52, 10.09s/it]\n",
      "Extracting crops: 18it [03:04, 10.56s/it]\n",
      "Extracting crops: 19it [03:15, 10.59s/it]\n",
      "Extracting crops: 20it [03:26, 10.88s/it]\n",
      "Extracting crops: 21it [03:38, 11.20s/it]\n",
      "Extracting crops: 22it [03:48, 10.79s/it]\n",
      "Extracting crops: 23it [03:58, 10.62s/it]\n",
      "Extracting crops: 24it [04:08, 10.36s/it]\n",
      "Extracting crops: 25it [04:20, 10.41s/it]\n",
      "\n",
      "Embedding extraction: 0it [00:00, ?it/s]\n",
      "Embedding extraction: 1it [00:12, 12.89s/it]\n",
      "Embedding extraction: 2it [00:24, 12.18s/it]\n",
      "Embedding extraction: 3it [00:36, 12.25s/it]\n",
      "Embedding extraction: 4it [00:49, 12.45s/it]\n",
      "Embedding extraction: 5it [01:06, 13.87s/it]\n",
      "Embedding extraction: 6it [01:30, 17.44s/it]\n",
      "Embedding extraction: 7it [01:49, 17.90s/it]\n",
      "Embedding extraction: 8it [02:04, 16.95s/it]\n",
      "Embedding extraction: 9it [02:17, 15.78s/it]\n",
      "Embedding extraction: 10it [02:32, 15.53s/it]\n",
      "Embedding extraction: 11it [02:46, 14.98s/it]\n",
      "Embedding extraction: 12it [03:02, 15.55s/it]\n",
      "Embedding extraction: 13it [03:20, 16.21s/it]\n",
      "Embedding extraction: 14it [03:36, 16.06s/it]\n",
      "Embedding extraction: 15it [03:50, 15.35s/it]\n",
      "FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "Embedding extraction: 0it [00:00, ?it/s]\n",
      "Embedding extraction: 1it [00:07,  7.48s/it]\n",
      "FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "Tracking ball possession: 3it [08:51, 177.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team 0 had possession for 1 frames.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "STRIDE = 30\n",
    "BALL_ID = 0\n",
    "PLAYER_ID = 2\n",
    "possession = track_ball_possession(SOURCE_VIDEO_PATH)\n",
    "\n",
    "for team_id, frames in possession.items():\n",
    "    print(f\"Team {team_id} had possession for {frames} frames.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
