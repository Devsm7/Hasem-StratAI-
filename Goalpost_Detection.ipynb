{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/05/25 13:15:20] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> Your inference package version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.43</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> is out of date! Please upgrade to <a href=\"file://c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\__init__.py#41\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.46</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> of inference for the latest features and bug fixes by    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         running `pip install --upgrade inference`.                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/05/25 13:15:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m Your inference package version \u001b[1;36m0.43\u001b[0m.\u001b[1;36m0\u001b[0m is out of date! Please upgrade to \u001b]8;id=240415;file://c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=231346;file://c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\__init__.py#41\u001b\\\u001b[2m41\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         version \u001b[1;36m0.46\u001b[0m.\u001b[1;36m0\u001b[0m of inference for the latest features and bug fixes by    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         running `pip install --upgrade inference`.                              \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "ModelDependencyMissing: Your `inference` configuration does not support PaliGemma model. Use pip install 'inference[transformers]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Florence2 model. Use pip install 'inference[transformers]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Qwen2.5-VL model. Use pip install 'inference[transformers]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[clip]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support OWLv2 model. Use pip install 'inference[transformers]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Gaze Detection model. Use pip install 'inference[gaze]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support GroundingDINO model. Use pip install 'inference[grounding-dino]' to install missing requirements.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support YoloWorld model. Use pip install 'inference[yolo-world]' to install missing requirements.\n",
      "UserWarning: Specified provider 'OpenVINOExecutionProvider' is not in available provider names.Available providers: 'TensorrtExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider'\n",
      "UserWarning: Specified provider 'CoreMLExecutionProvider' is not in available provider names.Available providers: 'TensorrtExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider'\n"
     ]
    }
   ],
   "source": [
    "from inference import get_model\n",
    "#from google.colab import userdata\n",
    "\n",
    "ROBOFLOW_API_KEY = \"ITfUpuY5QO9WTBpcEXTh\"  # Replace with your actual Roboflow API key\n",
    "\n",
    "GOAL_DETECTION_MODEL_ID = \"football-goalpost/3\"\n",
    "GOAL_DETECTION_MODEL = get_model(model_id=GOAL_DETECTION_MODEL_ID, api_key=ROBOFLOW_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "import numpy as np  \n",
    "\n",
    "SOURCE_VIDEO_PATH = \"C:/Users/user/Desktop/FootballAI/videos/goalMatch.mp4\"\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
    "    thickness=2\n",
    ")\n",
    "\n",
    "frame_number = 2000  # Change to the desired frame number\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "\n",
    "for i, frame in enumerate(frame_generator):\n",
    "    if i == frame_number:\n",
    "        # Run inference on the specific frame\n",
    "        result = GOAL_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
    "        detections = sv.Detections.from_inference(result)\n",
    "\n",
    "        label_annotator = sv.LabelAnnotator(##labeling the box and adding text\n",
    "          color = sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
    "          text_color = sv.Color.from_hex('#000000')\n",
    "           )\n",
    "        # Annotate the frame\n",
    "        annotated_frame = frame.copy()\n",
    "        annotated_frame = box_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=detections\n",
    "          )\n",
    "\n",
    "        labels = [\n",
    "           f\"{class_name} {confidence:.2f}\"\n",
    "          for class_name, confidence\n",
    "          in zip(detections[\"class_name\"] , detections.confidence)\n",
    "           ]\n",
    "\n",
    "        annotated_frame = label_annotator.annotate(annotated_frame , detections,labels =labels)##annotate the frame with the labels\n",
    " \n",
    "        # Show the annotated frame\n",
    "        sv.plot_image(annotated_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "SOURCE_VIDEO_PATH = \"C:/Users/user/Desktop/FootballAI/videos/goalMatch.mp4\"\n",
    "\n",
    "# Box annotator setup with color palette and thickness\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
    "    thickness=2\n",
    ")\n",
    "\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#FF8C00', '#00BFFF', '#FF1493', '#FFD700']),\n",
    "    text_color=sv.Color.from_hex('#000000')\n",
    ")\n",
    "\n",
    "\n",
    "# Frame generator from the video\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "\n",
    "# Define the font for text labels\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Loop through frames in the video\n",
    "for frame in frame_generator:\n",
    "    # Run inference on the current frame with confidence threshold\n",
    "    result = GOAL_DETECTION_MODEL.infer(frame, confidence=0.3)[0]\n",
    "    \n",
    "    # Convert the inference result to detections\n",
    "    detections = sv.Detections.from_inference(result)\n",
    "\n",
    "    # Copy the current frame for annotation\n",
    "    annotated_frame = frame.copy()\n",
    "    \n",
    "    # Annotate the frame with bounding boxes\n",
    "    annotated_frame = box_annotator.annotate(\n",
    "        scene=annotated_frame,\n",
    "        detections=detections\n",
    "    )\n",
    "    \n",
    "    labels = [\n",
    "          f\"{class_name} {confidence:.2f}\"\n",
    "          for class_name, confidence\n",
    "          in zip(detections[\"class_name\"] , detections.confidence)\n",
    "         ]\n",
    "    annotated_frame = label_annotator.annotate(annotated_frame , detections,labels =labels)##annotate the frame with the labels\n",
    "\n",
    "    # Show the annotated frame\n",
    "    sv.plot_image(annotated_frame)\n",
    "    break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")  # Make sure to replace with correct paths\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load class labels (COCO dataset, for example)\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load image (or use video frame)\n",
    "image = cv2.imread(\"image.jpg\")  # Replace with your image path\n",
    "height, width, channels = image.shape\n",
    "\n",
    "# Prepare the image for YOLO\n",
    "blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "net.setInput(blob)\n",
    "\n",
    "# Get the output from the YOLO network\n",
    "outs = net.forward(output_layers)\n",
    "\n",
    "# Initialize lists to hold the coordinates of detected objects\n",
    "coordinates = []\n",
    "\n",
    "# Loop over each output of the YOLO model\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]  # The class scores\n",
    "        class_id = np.argmax(scores)  # Get the class ID of the object\n",
    "        confidence = scores[class_id]  # Get the confidence of the prediction\n",
    "\n",
    "        # Only consider detections with high confidence\n",
    "        if confidence > 0.5:  # Confidence threshold (can be adjusted)\n",
    "            # Get the coordinates of the bounding box\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "\n",
    "            # Get top-left corner of the bounding box\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "\n",
    "            # Store the coordinates (x, y, w, h)\n",
    "            coordinates.append((x, y, w, h))\n",
    "\n",
    "# Print the coordinates of detected objects\n",
    "print(\"Detected Coordinates (x, y, w, h):\")\n",
    "for coord in coordinates:\n",
    "    print(coord)\n",
    "\n",
    "# Optionally, draw the bounding boxes on the image\n",
    "for coord in coordinates:\n",
    "    x, y, w, h = coord\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Show the result\n",
    "cv2.imshow(\"Image with Detected Objects\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_VIDEO_PATH = \"C:/Users/user/Desktop/FootballAI/videos/goalMatch.mp4\"\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)  # Get the first frame from the generator\n",
    "image = frame.copy()  # Copy the frame for processing\n",
    "\n",
    "# Resize the image to match the model's expected input dimensions (e.g., 640x640)\n",
    "image_resized = cv2.resize(image, (640, 640))\n",
    "\n",
    "# Normalize the image and convert to float32\n",
    "image_resized = image_resized.astype('float32') / 255.0\n",
    "\n",
    "# Reorder dimensions to match the model's expected input format (channels-first)\n",
    "image_resized = np.transpose(image_resized, (2, 0, 1))  # Convert from (640, 640, 3) to (3, 640, 640)\n",
    "\n",
    "# Add a batch dimension to the image\n",
    "image_resized = np.expand_dims(image_resized, axis=0)  # Convert to (1, 3, 640, 640)\n",
    "\n",
    "# Perform inference to get predictions\n",
    "# Perform inference to get predictions\n",
    "predictions = GOAL_DETECTION_MODEL.predict(image_resized, confidence=40, overlap=30)\n",
    "\n",
    "predicted_bboxes = []  # List to store predicted bounding boxes\n",
    "\n",
    "# Check the structure of predictions and extract bounding box and class details\n",
    "for prediction in predictions[0]:  # Access the first element of the predictions tuple\n",
    "    x, y, w, h = prediction[0], prediction[1], prediction[2], prediction[3]  # Adjust indices based on the actual structure\n",
    "    confidence = prediction[4]  # Assuming confidence is at index 4\n",
    "    class_id = prediction[5]  # Assuming class_id is at index 5\n",
    "    predicted_bboxes.append({\n",
    "        \"class_id\": class_id,\n",
    "        \"confidence\": confidence,\n",
    "        \"x\": x,\n",
    "        \"y\": y,\n",
    "        \"w\": w,\n",
    "        \"h\": h\n",
    "    })\n",
    "\n",
    "print(\"Saved Predictions:\")\n",
    "for bbox in predicted_bboxes:\n",
    "    print(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "SOURCE_VIDEO_PATH = \"C:/Users/user/Desktop/FootballAI/videos/goalMatch.mp4\"\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)  # Get the first frame from the generator\n",
    "image_path = \"C:/Users/user/Desktop/FootballAI/videos/goal1.png\"  # Path to the image\n",
    "image = cv2.imread(image_path)  # Load the image using OpenCV\n",
    "\n",
    "result = GOAL_DETECTION_MODEL.infer(image, confidence=0.3)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "\n",
    "# Resize the image to match the model's expected input dimensions (e.g., 1280x1280)\n",
    "image_resized = cv2.resize(image, (1280, 1280))\n",
    "\n",
    "# Normalize the image and convert to float32\n",
    "image_resized = image_resized.astype('float32') / 255.0\n",
    "\n",
    "# Reorder dimensions to match the model's expected input format (channels-first)\n",
    "image_resized = np.transpose(image_resized, (2, 0, 1))  # Convert from (640, 640, 3) to (3, 640, 640)\n",
    "\n",
    "# Add a batch dimension to the image\n",
    "image_resized = np.expand_dims(image_resized, axis=0)  # Convert to (1, 3, 640, 640)\n",
    "\n",
    "# Perform inference to get predictions\n",
    "# Ensure the image has the correct shape (height, width, channels)\n",
    "image_resized_corrected = np.transpose(image_resized[0], (1, 2, 0))  # Convert from (1, 3, 1280, 1280) to (1280, 1280, 3)\n",
    "\n",
    "# Perform inference to get predictions\n",
    "predictions = GOAL_DETECTION_MODEL.infer(image_resized_corrected, confidence=0.4)[0]  # Use the correct method and confidence threshold\n",
    "\n",
    "predicted_bboxes = []  # List to store predicted bounding boxes\n",
    "BALL_ID = 0  # Assuming class ID for ball is 0\n",
    "# Check the structure of predictions and extract bounding box and class details\n",
    "for detection, class_id in zip(detections.xyxy, detections.class_id):\n",
    "    if class_id == BALL_ID:  # Check if the detection corresponds to the ball\n",
    "        x_min, y_min, x_max, y_max = detection\n",
    "        ball_coords = ((x_min + x_max) / 2, (y_min + y_max) / 2)  # Get the center of the bounding box for the ball\n",
    "        print(f\"GOALPOST detected: {ball_coords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "SOURCE_VIDEO_PATH = \"C:/Users/user/Desktop/FootballAI/videos/goalMatch.mp4\"\n",
    "\n",
    "frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
    "frame = next(frame_generator)  # Get the first frame from the generator\n",
    "image_path = \"C:/Users/user/Desktop/FootballAI/videos/goal1.png\"  # Path to the image\n",
    "image = cv2.imread(image_path)  # Load the image using OpenCV\n",
    "\n",
    "# Resize the image to match the model's expected input dimensions (e.g., 1280x1280)\n",
    "image_resized = cv2.resize(image, (1280, 1280))\n",
    "\n",
    "# Normalize the image to [0, 1] range (if necessary)\n",
    "image_resized = image_resized.astype('float32') / 255.0\n",
    "\n",
    "# Reorder dimensions to match the model's expected input format (channels-first)\n",
    "image_resized = np.transpose(image_resized, (2, 0, 1))  # Convert from (1280, 1280, 3) to (3, 1280, 1280)\n",
    "\n",
    "# Add a batch dimension to the image\n",
    "image_resized = np.expand_dims(image_resized, axis=0)  # Convert to (1, 3, 1280, 1280)\n",
    "\n",
    "# Perform inference again with preprocessed image (remove batch dimension)\n",
    "image_resized_corrected = np.transpose(image_resized[0], (1, 2, 0))  # Convert from (1, 3, 1280, 1280) to (1280, 1280, 3)\n",
    "result = GOAL_DETECTION_MODEL.infer(image_resized_corrected, confidence=0.2)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "print(\"Detections:\", detections)\n",
    "\n",
    "# Perform inference to get predictions\n",
    "result = GOAL_DETECTION_MODEL.infer(image, confidence=0.3)[0]\n",
    "detections = sv.Detections.from_inference(result)\n",
    "\n",
    "# Print the inference result to check its structure\n",
    "print(\"Inference result:\", result)\n",
    "\n",
    "# Print the detections object to check the bounding boxes and class ids\n",
    "print(\"Detections:\", detections)\n",
    "\n",
    "# Extract bounding boxes and class IDs\n",
    "predicted_bboxes = []  # List to store predicted bounding boxes\n",
    "BALL_ID = 0  # Assuming class ID for ball is 0\n",
    "\n",
    "# Check the structure of predictions and extract bounding box and class details\n",
    "for detection, class_id in zip(detections.xyxy, detections.class_id):\n",
    "    print(f\"Detection: {detection}, Class ID: {class_id}\")  # Debugging output\n",
    "\n",
    "    if class_id == BALL_ID:  # Check if the detection corresponds to the ball\n",
    "        x_min, y_min, x_max, y_max = detection\n",
    "        ball_coords = ((x_min + x_max) / 2, (y_min + y_max) / 2)  # Get the center of the bounding box for the ball\n",
    "        print(f\"Ball detected: {ball_coords}\")\n",
    "\n",
    "# If no ball was detected, print a message\n",
    "if not predicted_bboxes:\n",
    "    print(\"No ball detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "# Function to calculate Euclidean distance\n",
    "def calculate_distance(ball_center, goalpost_center):\n",
    "    return sqrt((goalpost_center[0] - ball_center[0])**2 + (goalpost_center[1] - ball_center[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[116. 203. 212. 308.]]\n",
      "Goalpost center: (164.0, 255.5)\n"
     ]
    }
   ],
   "source": [
    "goalpost_center = None  # Variable to store the center of the goalpost\n",
    "\n",
    "image_path = \"C:/Users/user/Desktop/FootballAI/videos/image.png\"  # Path to the image\n",
    "image = cv2.imread(image_path)  # Load the image using OpenCV\n",
    "\n",
    "result = GOAL_DETECTION_MODEL.infer(image, confidence=0.3)[0]\n",
    "\n",
    "detections = sv.Detections.from_inference(result)\n",
    "print(detections.xyxy)\n",
    "\n",
    "for detection, class_id in zip(detections.xyxy, detections.class_id):\n",
    "        x_min, y_min, x_max, y_max = detection\n",
    "        goalpost_center = ((x_min + x_max) / 2, (y_min + y_max) / 2)  # Calculate the center of the goalpost\n",
    "        print(f\"Goalpost center: {goalpost_center}\")  # Print the center coordinates\n",
    "        break  # Stop after finding the first goalpost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Goalpost center: (164.0, 255.5)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "goalpost_center = None  # Variable to store the center of the goalpost\n",
    "\n",
    "image_path = \"C:/Users/user/Desktop/FootballAI/videos/image.png\"  # Path to the image\n",
    "image = cv2.imread(image_path)  # Load the image using OpenCV\n",
    "\n",
    "result = GOAL_DETECTION_MODEL.infer(image, confidence=0.3)[0]\n",
    "\n",
    "detections = sv.Detections.from_inference(result)\n",
    "print(detections.class_id)\n",
    "\n",
    "for detection, class_id in zip(detections.xyxy, detections.class_id):\n",
    "    x_min, y_min, x_max, y_max = detection\n",
    "    goalpost_center = ((x_min + x_max) / 2, (y_min + y_max) / 2)  # Calculate the center of the goalpost\n",
    "    print(f\"Goalpost center: {goalpost_center}\")  # Print the center coordinates\n",
    "    \n",
    "    # Draw a red dotted circle at the center\n",
    "    center_coordinates = (int(goalpost_center[0]), int(goalpost_center[1]))\n",
    "    radius = 5\n",
    "    color = (0, 0, 255)  # Red color in BGR\n",
    "    thickness = 2\n",
    "    line_type = cv2.LINE_AA  # Anti-aliased line for better quality\n",
    "    cv2.circle(image, center_coordinates, radius, color, thickness, lineType=line_type)\n",
    "    \n",
    "    break  # Stop after finding the first goalpost\n",
    "\n",
    "# Display the image with the red dotted circle\n",
    "cv2.imshow(\"Image with Center Point\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
